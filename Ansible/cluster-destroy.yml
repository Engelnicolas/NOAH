---
- name: Comprehensive NOAH Kubernetes Cluster Destruction
  hosts: localhost
  gather_facts: true
  vars:
    default_cluster_name: "noah-cluster"
    k3s_data_dir: "/var/lib/rancher/k3s"
    k3s_config_dir: "/etc/rancher/k3s"
    verbose_mode: true  # Enable verbose output by default
    system_namespaces:
      - kube-system
      - kube-public
      - kube-node-lease
      - default
    
  tasks:
    - name: Check if kubectl is available
      ansible.builtin.command: which kubectl
      register: kubectl_check
      ignore_errors: true
      changed_when: false

    - name: Check if K3s is running
      ansible.builtin.command: systemctl is-active k3s
      register: k3s_status
      ignore_errors: true
      changed_when: false

    - name: Display initial cluster state (verbose mode)
      ansible.builtin.debug:
        msg:
          - "[VERBOSE] Starting NOAH cluster destruction process"
          - "[VERBOSE] kubectl available: {{ kubectl_check.rc == 0 }}"
          - "[VERBOSE] K3s service active: {{ k3s_status.rc == 0 }}"
          - "[VERBOSE] Target cluster: {{ cluster_name | default(default_cluster_name) }}"
          - "[VERBOSE] Verbose mode: {{ verbose_mode }}"
      when: verbose_mode | default(true)

    - name: "PREREQUISITE: Delete all Helm releases (including NOAH components)"
      ansible.builtin.shell: |
        echo "=== DELETING ALL HELM RELEASES ==="
        helm list --all-namespaces -o json | \
        jq -r '.[] | "\(.name) \(.namespace)"' | \
        while read name namespace; do
          echo "Uninstalling Helm release: $name from namespace: $namespace"
          helm uninstall "$name" -n "$namespace" --timeout=120s --wait || true
        done
        echo "=== HELM CLEANUP COMPLETED ==="
      when: kubectl_check.rc == 0
      ignore_errors: true

    - name: "PREREQUISITE: Force delete all NOAH application pods"
      ansible.builtin.shell: |
        echo "=== DELETING NOAH PODS ==="
        # Delete pods with specific NOAH labels
        kubectl get pods --all-namespaces -l "noah.infra.com/managed-by=noah-cli" -o json 2>/dev/null | \
        jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | \
        while read namespace pod; do
          echo "Force deleting NOAH pod: $pod in namespace: $namespace"
          kubectl delete pod "$pod" -n "$namespace" --force --grace-period=0 || true
        done
        
        # Delete all pods in NOAH-related namespaces
        for ns in identity monitoring ingress-nginx cilium-system; do
          echo "Force deleting all pods in namespace: $ns"
          kubectl get pods -n "$ns" -o name 2>/dev/null | \
          xargs -r kubectl delete -n "$ns" --force --grace-period=0 || true
        done
        echo "=== NOAH PODS CLEANUP COMPLETED ==="
      when: kubectl_check.rc == 0
      ignore_errors: true

    - name: "PREREQUISITE: Delete all NOAH secrets and configmaps"
      ansible.builtin.shell: |
        echo "=== DELETING NOAH SECRETS AND CONFIGMAPS ==="
        # Delete NOAH-specific secrets
        kubectl get secrets --all-namespaces -l "noah.infra.com/component" -o json 2>/dev/null | \
        jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | \
        while read namespace secret; do
          echo "Deleting NOAH secret: $secret from namespace: $namespace"
          kubectl delete secret "$secret" -n "$namespace" || true
        done
        
        # Delete NOAH-specific configmaps
        kubectl get configmaps --all-namespaces -l "noah.infra.com/component" -o json 2>/dev/null | \
        jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | \
        while read namespace cm; do
          echo "Deleting NOAH configmap: $cm from namespace: $namespace"
          kubectl delete configmap "$cm" -n "$namespace" || true
        done
        
        # Delete specific NOAH secrets by name pattern
        for pattern in authentik samba4 cilium noah; do
          kubectl get secrets --all-namespaces --field-selector metadata.name~="$pattern" -o json 2>/dev/null | \
          jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | \
          while read namespace secret; do
            echo "Deleting secret by pattern: $secret from namespace: $namespace"
            kubectl delete secret "$secret" -n "$namespace" || true
          done
        done
        echo "=== SECRETS AND CONFIGMAPS CLEANUP COMPLETED ==="
      when: kubectl_check.rc == 0
      ignore_errors: true

    - name: "PREREQUISITE: Delete all non-system namespaces"
      ansible.builtin.shell: |
        echo "=== DELETING NON-SYSTEM NAMESPACES ==="
        kubectl get namespaces -o json | \
        jq -r '.items[] | select(.metadata.name | test("^(kube-system|kube-public|kube-node-lease|default)$") | not) | .metadata.name' | \
        while read namespace; do
          echo "Deleting namespace: $namespace"
          kubectl delete namespace "$namespace" --timeout=60s --force --grace-period=0 || true
        done
        echo "=== NAMESPACE CLEANUP COMPLETED ==="
      when: kubectl_check.rc == 0
      ignore_errors: true

    - name: "PREREQUISITE: Validate clean state"
      ansible.builtin.shell: |
        echo "=== VALIDATING CLUSTER CLEAN STATE ==="
        
        # Check for remaining NOAH pods
        remaining_pods=$(kubectl get pods --all-namespaces --field-selector metadata.namespace!=kube-system,metadata.namespace!=kube-public,metadata.namespace!=kube-node-lease,metadata.namespace!=default -o name 2>/dev/null | wc -l)
        echo "Remaining non-system pods: $remaining_pods"
        
        # Check for remaining Helm releases
        remaining_releases=$(helm list --all-namespaces -o json | jq '. | length')
        echo "Remaining Helm releases: $remaining_releases"
        
        # Check for remaining non-system namespaces
        remaining_namespaces=$(kubectl get namespaces -o json | jq -r '.items[] | select(.metadata.name | test("^(kube-system|kube-public|kube-node-lease|default)$") | not) | .metadata.name' | wc -l)
        echo "Remaining non-system namespaces: $remaining_namespaces"
        
        echo "=== CLUSTER STATE VALIDATION COMPLETED ==="
        echo "Prerequisites met: Cluster is clean and ready for redeployment"
      when: kubectl_check.rc == 0
      ignore_errors: true
      register: validation_result

    - name: Stop K3s service
      ansible.builtin.systemd:
        name: k3s
        state: stopped
        enabled: false
      ignore_errors: true
      when: k3s_status.rc == 0

    - name: Display K3s service stop result (verbose mode)
      ansible.builtin.debug:
        msg: "[VERBOSE] K3s service stop attempted for active service"
      when: verbose_mode | default(true) and k3s_status.rc == 0

    - name: Kill any remaining K3s processes
      ansible.builtin.shell: |
        echo "[VERBOSE] Checking for K3s processes to terminate..."
        k3s_pids=$(pgrep -f k3s || true)
        if [ -n "$k3s_pids" ]; then
          echo "[VERBOSE] Found K3s processes: $k3s_pids"
          echo "[VERBOSE] Sending TERM signal to K3s processes..."
          pkill -f k3s || true
          echo "[VERBOSE] Sending TERM signal to containerd-shim processes..."
          pkill -f containerd-shim || true
          echo "[VERBOSE] Waiting 5 seconds for graceful shutdown..."
          sleep 5
          echo "[VERBOSE] Sending KILL signal to any remaining K3s processes..."
          pkill -9 -f k3s || true
          echo "[VERBOSE] K3s process cleanup completed"
        else
          echo "[VERBOSE] No K3s processes found to terminate"
        fi
      ignore_errors: true

    - name: Unmount K3s filesystems
      ansible.builtin.shell: |
        echo "[VERBOSE] Checking for K3s filesystem mounts..."
        k3s_mounts=$(mount | grep -E '/var/lib/rancher/k3s|/run/k3s' || true)
        if [ -n "$k3s_mounts" ]; then
          echo "[VERBOSE] Found K3s mounts to unmount:"
          echo "$k3s_mounts"
          echo "[VERBOSE] Unmounting K3s filesystems..."
          umount $(mount | grep '/var/lib/rancher/k3s' | awk '{print $3}') 2>/dev/null || true
          umount $(mount | grep '/run/k3s' | awk '{print $3}') 2>/dev/null || true
          echo "[VERBOSE] K3s filesystem unmount completed"
        else
          echo "[VERBOSE] No K3s filesystem mounts found"
        fi
      ignore_errors: true

    - name: Remove K3s data directories (preserve service files and binary)
      ansible.builtin.file:
        path: "{{ item }}"
        state: absent
      loop:
        - "{{ k3s_data_dir }}"
        - "{{ k3s_config_dir }}"
        - "/run/k3s"
        - "/var/lib/kubelet"
        - "/var/lib/cni"
        - "/opt/cni"
      ignore_errors: true

    - name: Display data directory cleanup status (verbose mode)
      ansible.builtin.debug:
        msg:
          - "[VERBOSE] K3s data directory cleanup completed"
          - "[VERBOSE] Removed directories:"
          - "  - {{ k3s_data_dir }}"
          - "  - {{ k3s_config_dir }}"
          - "  - /run/k3s"
          - "  - /var/lib/kubelet"
          - "  - /var/lib/cni"
          - "  - /opt/cni"
          - "[VERBOSE] Preserved files:"
          - "  - /usr/local/bin/k3s (binary)"
          - "  - /etc/systemd/system/k3s*.service (service files)"
      when: verbose_mode | default(true)

    # Note: Keeping K3s service files to avoid need for reinstallation
    # - Service files will be reused on cluster recreation
    # - Only data directories are removed for clean state

    - name: Reload systemd daemon
      ansible.builtin.systemd:
        daemon_reload: yes
      ignore_errors: true
      become: yes

    - name: Clean container images (optional)
      ansible.builtin.shell: |
        # Clean up any remaining container images
        if command -v crictl >/dev/null 2>&1; then
          crictl rmi --prune || true
        fi
        if command -v docker >/dev/null 2>&1; then
          docker system prune -af || true
        fi
      ignore_errors: true

    - name: Final validation - Verify complete destruction
      ansible.builtin.shell: |
        echo "=== FINAL DESTRUCTION VALIDATION ==="
        
        # Check if K3s is still running
        if systemctl is-active k3s >/dev/null 2>&1; then
          echo "WARNING: K3s service is still active"
        else
          echo "‚úì K3s service stopped"
        fi
        
        # Check for remaining K3s processes
        k3s_processes=$(pgrep -f k3s | wc -l)
        echo "Remaining K3s processes: $k3s_processes"
        
        # Check for data directories
        for dir in "{{ k3s_data_dir }}" "{{ k3s_config_dir }}" "/run/k3s"; do
          if [ -d "$dir" ]; then
            echo "WARNING: Directory still exists: $dir"
          else
            echo "‚úì Directory removed: $dir"
          fi
        done
        
        echo "=== DESTRUCTION VALIDATION COMPLETED ==="
        echo "Cluster destruction completed successfully"
        echo "K3s binary and service files preserved for reuse"
        echo "System is ready for fresh cluster creation"
      ignore_errors: true

    - name: "COMPREHENSIVE CLUSTER VALIDATION: Verify no K3s cluster exists"
      ansible.builtin.shell: |
        echo ""
        echo "=============================================="
        echo "  COMPREHENSIVE K3S CLUSTER VALIDATION TEST  "
        echo "=============================================="
        echo ""
        
        validation_errors=0
        
        echo "[VERBOSE] Testing K3s service status..."
        if systemctl is-active k3s >/dev/null 2>&1; then
          echo "‚ùå FAIL: K3s service is still running"
          systemctl status k3s --no-pager -l || true
          validation_errors=$((validation_errors + 1))
        else
          echo "‚úÖ PASS: K3s service is stopped"
        fi
        
        echo ""
        echo "[VERBOSE] Testing K3s process cleanup..."
        k3s_processes=$(pgrep -f k3s 2>/dev/null || true)
        if [ -n "$k3s_processes" ]; then
          echo "‚ùå FAIL: K3s processes still running:"
          echo "$k3s_processes" | while read pid; do
            ps -p $pid -o pid,ppid,cmd --no-headers 2>/dev/null || true
          done
          validation_errors=$((validation_errors + 1))
        else
          echo "‚úÖ PASS: No K3s processes running"
        fi
        
        echo ""
        echo "[VERBOSE] Testing kubectl cluster connectivity..."
        if timeout 5s kubectl cluster-info >/dev/null 2>&1; then
          echo "‚ùå FAIL: kubectl can still connect to a cluster"
          kubectl cluster-info 2>/dev/null || true
          validation_errors=$((validation_errors + 1))
        else
          echo "‚úÖ PASS: kubectl cannot connect to any cluster"
        fi
        
        echo ""
        echo "[VERBOSE] Testing Kubernetes API accessibility..."
        if timeout 3s curl -k https://127.0.0.1:6443/api >/dev/null 2>&1; then
          echo "‚ùå FAIL: Kubernetes API is still accessible on localhost:6443"
          validation_errors=$((validation_errors + 1))
        else
          echo "‚úÖ PASS: Kubernetes API is not accessible"
        fi
        
        echo ""
        echo "[VERBOSE] Testing data directory cleanup..."
        for dir in "{{ k3s_data_dir }}" "{{ k3s_config_dir }}" "/run/k3s" "/var/lib/kubelet" "/var/lib/cni"; do
          if [ -d "$dir" ]; then
            echo "‚ùå FAIL: Directory still exists: $dir"
            echo "  Contents: $(ls -la "$dir" 2>/dev/null | head -5)" 
            validation_errors=$((validation_errors + 1))
          else
            echo "‚úÖ PASS: Directory removed: $dir"
          fi
        done
        
        echo ""
        echo "[VERBOSE] Testing Helm releases cleanup..."
        if command -v helm >/dev/null 2>&1; then
          remaining_releases=$(helm list --all-namespaces -o json 2>/dev/null | jq '. | length' 2>/dev/null || echo "0")
          if [ "$remaining_releases" -gt 0 ]; then
            echo "‚ùå FAIL: $remaining_releases Helm releases still exist:"
            helm list --all-namespaces 2>/dev/null || true
            validation_errors=$((validation_errors + 1))
          else
            echo "‚úÖ PASS: All Helm releases removed"
          fi
        else
          echo "‚ö†Ô∏è  SKIP: Helm not available for validation"
        fi
        
        echo ""
        echo "[VERBOSE] Testing container runtime cleanup..."
        if command -v crictl >/dev/null 2>&1; then
          running_containers=$(crictl ps -q 2>/dev/null | wc -l)
          if [ "$running_containers" -gt 0 ]; then
            echo "‚ùå FAIL: $running_containers containers still running:"
            crictl ps 2>/dev/null || true
            validation_errors=$((validation_errors + 1))
          else
            echo "‚úÖ PASS: No containers running"
          fi
        else
          echo "‚ö†Ô∏è  SKIP: crictl not available for container validation"
        fi
        
        echo ""
        echo "[VERBOSE] Testing network interface cleanup..."
        k3s_interfaces=$(ip link show 2>/dev/null | grep -E "(cni0|flannel|veth)" | wc -l)
        if [ "$k3s_interfaces" -gt 0 ]; then
          echo "‚ö†Ô∏è  WARNING: K3s network interfaces may still exist:"
          ip link show 2>/dev/null | grep -E "(cni0|flannel|veth)" || true
          echo "  This is usually not critical and will be cleaned up on reboot"
        else
          echo "‚úÖ PASS: No K3s network interfaces detected"
        fi
        
        echo ""
        echo "[VERBOSE] Testing file system mounts..."
        k3s_mounts=$(mount | grep -E "(k3s|kubelet)" | wc -l)
        if [ "$k3s_mounts" -gt 0 ]; then
          echo "‚ùå FAIL: K3s file system mounts still exist:"
          mount | grep -E "(k3s|kubelet)" || true
          validation_errors=$((validation_errors + 1))
        else
          echo "‚úÖ PASS: No K3s file system mounts"
        fi
        
        echo ""
        echo "=============================================="
        echo "           VALIDATION SUMMARY"
        echo "=============================================="
        
        if [ $validation_errors -eq 0 ]; then
          echo "üéâ SUCCESS: K3s cluster completely destroyed"
          echo "‚úÖ All validation tests passed"
          echo "‚úÖ System is clean and ready for fresh deployment"
          echo ""
          echo "Next step: python noah.py cluster create --name <cluster-name>"
          exit 0
        else
          echo "‚ùå FAILURE: $validation_errors validation errors found"
          echo "‚ö†Ô∏è  Manual intervention may be required"
          echo ""
          echo "Troubleshooting steps:"
          echo "1. Check running processes: ps aux | grep k3s"
          echo "2. Check system services: systemctl list-units | grep k3s"
          echo "3. Check mount points: mount | grep k3s"
          echo "4. Reboot system if necessary to clear all resources"
          echo ""
          exit 1
        fi
      register: comprehensive_validation
      failed_when: comprehensive_validation.rc != 0

    - name: Display destruction summary
      debug:
        msg:
          - "=== NOAH CLUSTER DESTRUCTION COMPLETE ==="
          - "‚úì All Helm releases uninstalled"
          - "‚úì All NOAH pods and secrets removed"  
          - "‚úì All non-system namespaces deleted"
          - "‚úì K3s service stopped and cluster data removed"
          - "‚úì K3s binary and service files preserved for reuse"
          - "‚úì System validated and ready for redeployment"
          - ""
          - "Next step: Run 'python noah.py cluster create' to create a fresh cluster"
